import gym
import numpy as np
import torch
import json
import time
from datetime import datetime
import os
from PIL import Image
import matplotlib.pyplot as plt
from homegrid.BLIP import BLIP2Helper
import random


class BLIPAgent:
    def __init__(self, env_name="homegrid-task", checkpoint_dir=None):
        """
        Initialize a BLIP-based agent for HomegGrid environment.
        """
        assert checkpoint_dir is not None, "checkpoint_dir must be provided"

        # Initialize environment
        self.env = gym.make(env_name, disable_env_checker=True)
        self.action_space = self.env.action_space.n

        # Initialize BLIP helper
        self.llm_helper = BLIP2Helper()

        # Define action mappings
        self.action_map = {
            0: "left",
            1: "right",
            2: "up",
            3: "down",
            4: "pickup",
            5: "drop",
            6: "get",
            7: "pedal",
            8: "grasp",
            9: "lift",
        }

        # Create directories for results
        self.checkpoint_dir = checkpoint_dir
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        self.results_dir = os.path.join(self.checkpoint_dir, "blip_results")
        os.makedirs(self.results_dir, exist_ok=True)

        # Performance metrics
        self.metrics = {
            "success_rate": 0,
            "avg_steps": 0,
            "avg_uncertainty": 0,
            "tasks": {},
        }

    def format_context(self, info):
        """
        Format the symbolic state information into a context string for BLIP.
        """
        agent_info = info["symbolic_state"]["agent"]

        context_dict = {
            "direction": self.direction_to_str(agent_info["dir"]),
            "carrying object": agent_info.get("carrying", "none"),
            "front object": info["symbolic_state"].get("front_obj", "none"),
            "task": self.env.task,
            "prior VLM output": "",
        }

        return json.dumps(context_dict)

    def direction_to_str(self, dir_int):
        """Convert direction integer to string representation."""
        directions = ["right", "down", "left", "up"]
        return directions[dir_int]

    def select_action(self, hint):
        """
        Parse the hint from BLIP and select an appropriate action.

        Args:
            hint (str): The hint generated by BLIP model

        Returns:
            int: Action index to take
        """
        hint_lower = hint.lower()

        # Map keywords to actions
        action_keywords = {
            "left": 0,
            "right": 1,
            "up": 2,
            "down": 3,
            "pickup": 4,
            "pick up": 4,
            "take": 4,
            "grab": 4,
            "collect": 4,
            "drop": 5,
            "put down": 5,
            "release": 5,
            "place": 5,
            "get": 6,
            "obtain": 6,
            "acquire": 6,
            "pedal": 7,
            "grasp": 8,
            "hold": 8,
            "lift": 9,
            "raise": 9,
            "elevate": 9,
        }

        # Check for action keywords in the hint
        for keyword, action in action_keywords.items():
            if keyword in hint_lower:
                return action

        # If no clear action found, check for directional context
        for keyword in ["go", "move", "head", "proceed", "navigate"]:
            if keyword in hint_lower:
                if "left" in hint_lower:
                    return 0
                elif "right" in hint_lower:
                    return 1
                elif "up" in hint_lower or "forward" in hint_lower:
                    return 2
                elif "down" in hint_lower or "back" in hint_lower:
                    return 3

        # If still no clear action, return random action as fallback
        return random.randint(0, self.action_space - 1)

    def run_episode(self, render=False):
        """
        Run a single episode using BLIP to generate actions.

        Returns:
            dict: Episode results including rewards, steps and success
        """
        obs, info = self.env.reset()
        task = self.env.task

        total_reward = 0
        steps = 0
        done = False
        uncertainties = []
        hints = []

        while not done and steps < self.env.max_steps:
            # Get image from observation
            pil_image = Image.fromarray(obs["image"])

            # Format context
            context = self.format_context(info)

            # Query BLIP for hint
            hint, uncertainty = self.llm_helper.query_llm(pil_image, context)
            hints.append(hint)
            uncertainties.append(uncertainty)

            # Choose action based on hint
            action = self.select_action(hint)

            # Take action in environment
            obs, reward, terminated, truncated, info = self.env.step(action)

            total_reward += reward
            steps += 1

            if render:
                self.env.render()
                print(
                    f"Step {steps}: Hint: '{hint}', Action: {self.action_map[action]}"
                )
                print(f"Uncertainty: {uncertainty:.4f}, Reward: {reward}")

            if terminated or truncated:
                done = True

        # Episode results
        success = total_reward > 0

        return {
            "task": task,
            "success": success,
            "reward": total_reward,
            "steps": steps,
            "hints": hints,
            "uncertainties": uncertainties,
            "avg_uncertainty": (
                sum(uncertainties) / len(uncertainties) if uncertainties else 0
            ),
        }

    def test(self, episodes=100, render=False):
        """
        Test the BLIP agent over multiple episodes and collect metrics.

        Args:
            episodes (int): Number of episodes to run
            render (bool): Whether to render the environment

        Returns:
            dict: Testing metrics
        """
        print(f"Testing BLIP agent for {episodes} episodes...")
        start_time = time.time()

        successful_episodes = 0
        all_steps = []
        all_uncertainties = []
        task_results = {}

        for ep in range(episodes):
            # Run episode
            result = self.run_episode(render)
            all_steps.append(result["steps"])
            all_uncertainties.append(result["avg_uncertainty"])

            # Update task-specific results
            task = result["task"]
            if task not in task_results:
                task_results[task] = {"attempts": 0, "successes": 0, "rewards": []}

            task_results[task]["attempts"] += 1
            task_results[task]["rewards"].append(result["reward"])

            if result["success"]:
                successful_episodes += 1
                task_results[task]["successes"] += 1

            # Print progress
            if (ep + 1) % 10 == 0 or ep == 0 or ep == episodes - 1:
                print(
                    f"Episode {ep+1}/{episodes}, Task: {task}, Success: {result['success']}, "
                    f"Steps: {result['steps']}, Avg Uncertainty: {result['avg_uncertainty']:.4f}"
                )

        # Calculate overall metrics
        test_time = time.time() - start_time
        success_rate = (successful_episodes / episodes) * 100
        avg_steps = sum(all_steps) / len(all_steps)
        avg_uncertainty = sum(all_uncertainties) / len(all_uncertainties)

        # Calculate task-specific success rates
        for task, data in task_results.items():
            data["success_rate"] = (data["successes"] / data["attempts"]) * 100
            data["avg_reward"] = sum(data["rewards"]) / len(data["rewards"])

        # Store metrics
        self.metrics = {
            "success_rate": success_rate,
            "avg_steps": avg_steps,
            "avg_uncertainty": avg_uncertainty,
            "tasks": task_results,
            "test_time": test_time,
            "episodes": episodes,
            "timestamp": datetime.now().strftime("%Y%m%d_%H%M%S"),
        }

        # Print results
        print("\n" + "=" * 50)
        print("BLIP AGENT TEST RESULTS")
        print("=" * 50)
        print(f"Total episodes: {episodes}")
        print(f"Success rate: {success_rate:.2f}%")
        print(f"Average steps: {avg_steps:.2f}")
        print(f"Average uncertainty: {avg_uncertainty:.4f}")
        print(f"Test time: {test_time:.2f} seconds")

        print("\nTask-specific success rates:")
        for task, data in task_results.items():
            print(
                f"  {task}: {data['success_rate']:.2f}% ({data['successes']}/{data['attempts']})"
            )

        # Save results
        self.save_results()

        return self.metrics

    def save_results(self):
        """Save test results to file."""
        timestamp = self.metrics["timestamp"]

        # Save JSON results
        results_path = os.path.join(self.results_dir, f"blip_results_{timestamp}.json")
        with open(results_path, "w") as f:
            json.dump(self.metrics, f, indent=4)

        # Generate and save plots
        self.generate_plots(timestamp)

        print(f"\nResults saved to: {results_path}")

    def generate_plots(self, timestamp):
        """Generate and save performance plots."""
        plt.figure(figsize=(12, 8))

        # Task success rates plot
        plt.subplot(1, 2, 1)
        tasks = list(self.metrics["tasks"].keys())
        success_rates = [self.metrics["tasks"][t]["success_rate"] for t in tasks]

        # Sort by success rate for better visualization
        sorted_indices = np.argsort(success_rates)
        sorted_tasks = [tasks[i] for i in sorted_indices]
        sorted_rates = [success_rates[i] for i in sorted_indices]

        plt.barh(sorted_tasks, sorted_rates, color="skyblue")
        plt.xlabel("Success Rate (%)")
        plt.title("Task-specific Success Rates")
        plt.grid(axis="x", linestyle="--", alpha=0.7)

        # Add text labels
        for i, v in enumerate(sorted_rates):
            plt.text(v + 1, i, f"{v:.1f}%", va="center")

        # Overall metrics plot
        plt.subplot(1, 2, 2)
        metrics = ["Success Rate", "Avg Steps", "Avg Uncertainty"]
        values = [
            self.metrics["success_rate"],
            min(100, self.metrics["avg_steps"]),
            self.metrics["avg_uncertainty"] * 100,  # Scale for visibility
        ]

        plt.bar(metrics, values, color=["green", "orange", "red"])
        plt.ylabel("Value")
        plt.title("Overall Performance Metrics")
        plt.xticks(rotation=30)
        plt.grid(axis="y", linestyle="--", alpha=0.7)

        # Add text labels
        plt.text(0, values[0] + 2, f"{values[0]:.1f}%", ha="center")
        plt.text(1, values[1] + 2, f"{self.metrics['avg_steps']:.1f}", ha="center")
        plt.text(
            2, values[2] + 2, f"{self.metrics['avg_uncertainty']:.3f}", ha="center"
        )

        plt.tight_layout()
        plot_path = os.path.join(self.results_dir, f"blip_performance_{timestamp}.png")
        plt.savefig(plot_path)
        plt.close()


if __name__ == "__main__":
    # Directory to save results
    checkpoint_dir = "blip_checkpoints"

    # Create and test BLIP agent
    agent = BLIPAgent(env_name="homegrid-task", checkpoint_dir=checkpoint_dir)

    # Test with 100 episodes
    agent.test(episodes=100, render=False)
